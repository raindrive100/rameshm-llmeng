# RAG Pipeline Implementation

A complete Retrieval-Augmented Generation (RAG) pipeline Python implementation using HotpotQA dataset, ChromaDB for vector storage, and multiple LLM providers.

## Features

- **Multiple Embedding Models**: Support for OpenAI, Google, and Open-source local embedding models
- **Multiple LLM Providers**: OpenAI GPT, Google Gemini, Anthropic Claude, and Open-source local Ollama model llama3.2
- **Hotpot QA dataset from Hugging Face**: Refer to Hugging Face for detailed documentation for the dataset (https://huggingface.co/datasets/hotpotqa/hotpot_qa)
- **Persistent Vector Storage**: ChromaDB with disk persistence and cosine similarity search
- **Configurable Pipeline**: YAML configuration for easy parameter tuning
- **Modular Architecture**: Extensible factory pattern for adding new models
- **Production Ready**: Configured with basic logging and error handling. Fine tune to your needs.

## File Contents
***The files generated by LLMs have been modified to get them into runnable state. All changes made are indicated with comments starting with "RRM Code"***
- **rag_code_gen_prompts.txt**: Prompts for generating RAG Pipeline Python Files using LLMs.
- **rag_example_with_chromadb.py**: A good working example of RAG Pipeline that I coded. Good for comparision with the code developed by LLMs.
- **rag_example_sonnet4_with_chromadb.py**: Code generated by Sonnet4 LLM model from Claude. Code changes I made to work are commented with "RRM Code"
- **rag_example_gpt5thinking_with_chromadb.py**: Code generated by GPT5 LLM model from OpenAI. Code changes I made to work are commented with "RRM Code"
- **rag_example_gpt5thinking_with_chromadb.py**: Code generated by Gemini 2.5 LLM model from Google. Code changes I made to work are commented with "RRM Code"
- **.YAML Files** - The yaml configuration files used by the corresponding .py files.
## Installation

1. **Install Dependencies**:

   ```
   pip install pip-tools
   pip-compile requirements.in --resolver=backtracking  # Prepares the requirements.txt that is appropriate for your python version
   pip install -r requirements.txt
   ```

2. **Set Environment Variables**:
    Place your API Keys for Gemini, OpenAI, and Claude in **.env** file and make it accessible to be read by **load_dotenv() python module**
   (or) "export" each API key as below:

      ```
      # For OpenAI models
      export OPENAI_API_KEY="your_openai_key"

      # For Google Gemini models
      export GOOGLE_API_KEY="your_google_key"

      # For Anthropic Claude models
      export ANTHROPIC_API_KEY="your_claude_key"

      # For local Ollama (optional)
      export OLLAMA_API_KEY="ollama"
      ```

3. **For Local Ollama Setup** (optional): Need only if you want to run using llama3.2 models etc..

   ```
   # Install Ollama (follow instructions at https://ollama.com)
   curl -fsSL https://ollama.ai/install.sh | sh

   # Pull LLaMA model
   ollama pull llama3.2

   # Start Ollama server (runs on localhost:11434)
   ollama serve
   ```

## Usage

1. **Run the Pipeline**:
   
   *If you want to run the code generated by each LLM then: Replace "rag_example_with_chromadb.py" in the below command
   with any of rag_example_sonnet4_with_chromadb.py or rag_example_gpt5thinking_with_chromadb.py or
   rag_example_gemin25_with_chromadb.py*

   ```
   python rag_example_with_chromadb.py
   ```

2. **Follow Interactive Prompts**: To chose the embedding model and LLM Model and input your query from the hotpot-qa.
   
   **Some example queries and answers you can leverage are provided below:**
   
   - **Question**: Which magazine was started first Arthur's Magazine or First for Women?  
     **Answer**: Arthur's Magazine

   - **Question**: The Oberoi family is part of a hotel company that has a head office in what city?  
     **Answer**: Delhi

   - **Question**: Musician and satirist Allie Goertz wrote a song about the "The Simpsons" character Milhouse, who Matt Groening named after who?  
     **Answer**: President Richard Nixon

   - **Question**: What nationality was James Henry Miller's wife?  
     **Answer**: American

   - **Question**: Cadmium Chloride is slightly soluble in this chemical, it is also called what?  
     **Answer**: alcohol

   - **Question**: Which tennis player won more Grand Slam titles, Henri Leconte or Jonathan Stark?  
     **Answer**: Jonathan Stark

   - **Question**: Which genus of moth in the world's seventh-largest country contains only one species?  
     **Answer**: Crambidae

   - **Question**: Who was once considered the best kick boxer in the world, however he has been involved in a number of controversies relating to his "unsportsmanlike conducts" in the sport and crimes of violence outside of the ring?  
     **Answer**: Badr Hari

   - **Question**: The Dutch-Belgian television series that "House of Anubis" was based on first aired in what year?  
     **Answer**: 2006

   - **Question**: What is the length of the track where the 2013 Liqui Moly Bathurst 12 Hour was staged?  
     **Answer**: 6.213 km long

3. **Example Interaction**:

   3.1: Input the preferred logging level to be one of: DEBUG | Info | Error: **Debug**

   3.2: Select embedding model:
   ```
   1: gemini-embedding-001 (Google's Gemini embedding model)
   2: text-embedding-3-small (OpenAI's embedding model)
   3: all-MiniLM-L6-v2 (ChromaDB built-in sentence transformer)
   Enter your choice number: 1
   ```

   3.3: Select LLM model:
   ```
   1: gpt-4o-mini (OpenAI GPT-4o-mini)
   2: gemini-1.5-flash (Google Gemini 1.5 Flash)
   3: llama3.2 (Meta LLaMA 3.2 (local via Ollama))
   4: claude-sonnet-4-20250514 (Claude Sonnet 4)
   Enter your choice number: 1
   ```

   3.4: Ready for queries. Type 'quit' to exit.
   ```
   Enter your question:
   ```

## Troubleshooting

### Common Issues

1. **Missing API Keys**: Ensure all required environment variables are set
2. **ChromaDB Errors**: Check write permissions for the vector store path
3. **Memory Issues**: Reduce batch size in `VectorStore.add_documents()`
4. **Ollama Connection**: Ensure Ollama server is running on localhost:11434

### Debug Mode
Enable detailed logging by modifying the logging level.

## Dataset Information

This implementation uses the HotpotQA dataset:
- **Dataset**: `hotpot_qa` from Hugging Face
- **Split**: `train` split from `distractor` configuration
- **Processing**: Context titles and sentences are concatenated with proper formatting
- **Metadata**: Includes type and level information for each context

## Performance Considerations

- **Batch Processing**: Documents are processed in batches to manage memory
- **Context Length**: Context is truncated to prevent LLM token limits
- **Persistence**: ChromaDB data persists between runs for faster startup
- **Similarity Metric**: Uses cosine similarity for optimal semantic matching

## License

This implementation is provided as-is for educational and research purposes. Please ensure compliance with the respective APIs' terms of service when using commercial LLM providers.