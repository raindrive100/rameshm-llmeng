{
 "cells": [
  {
   "cell_type": "code",
   "id": "a653e8a9-9917-4623-a197-f9f52cc4cf2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:11:32.060180Z",
     "start_time": "2025-06-06T20:11:32.055816Z"
    }
   },
   "source": [
    "# This is a simple general-purpose chatbot built on top of LangChain and Gradio.\n",
    "# Before running this, make sure you have:\n",
    "# 1. retrieved Keys for the LLM model environment you want to run\n",
    "# 2. set environment variable LLM_KEY_FILE pointing to the full path that contains the keys for the LLM Models."
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "af84394f-f1ad-46e3-83c4-4bf22e42e5fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:21:40.261885Z",
     "start_time": "2025-06-06T20:21:40.245277Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "import gradio as gr\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from rameshm.llmeng.utils import init_utils\n",
    "from rameshm.llmeng.utils.init_utils import set_environment_logger"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "a732616a-8b2b-453d-8cac-764c9eefb4b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:21:43.263608Z",
     "start_time": "2025-06-06T20:21:43.245852Z"
    }
   },
   "source": [
    "# Load my environment\n",
    "logger = set_environment_logger()\n",
    "print(f\"OPENAI_API_KEY: {os.getenv('OPENAI_API_KEY')}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log File: c:\\temp\\my_logs.txt\n",
      "OPENAI_API_KEY: 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\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "09b74bb1-c4a0-411c-b5b9-54259af61b80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:22:00.253750Z",
     "start_time": "2025-06-06T20:22:00.244660Z"
    }
   },
   "source": [
    "def get_model(model_nm: str):\n",
    "    model_nm = model_nm.split(\" \")[1].strip()\n",
    "    print(f\"Using Model: {model_nm}\")\n",
    "    if \"gpt\" in model_nm:\n",
    "        return ChatOpenAI(model=model_nm, api_key=os.getenv(\"OPENAI_API_KEY\"), temperature=0.7, timeout=30)\n",
    "    elif \"claude\" in model_nm:\n",
    "        return ChatAnthropic(model=model_nm, api_key=os.getenv(\"ANTHROPIC_API_KEY\"), timeout=30,\n",
    "                             temperature=0.7, max_tokens=1024,top_p=0.9, top_k=40)\n",
    "    elif \"llama\" in model_nm or \"gemma\" in model_nm:\n",
    "        # Ollama run on \"http://localhost:11434\"  # Default Ollama URL. If you type that URL you shoud see \"Ollama Running\" message\n",
    "        return Ollama(model=model_nm, # api_key=\"ollama\",base_url=\"http://localhost:11434\", \n",
    "                      temperature=0.7, top_p=0.9, top_k=40, num_predict=256, repeat_penalty=1.1)\n",
    "    elif \"gemini\" in model_nm:\n",
    "         return ChatGoogleGenerativeAI(model=model_nm, google_api_key=os.getenv(\"GOOGLE_API_KEY\"), timeout=30)\n",
    "    else:\n",
    "        raise Exception(\"Model: {model_nm} is not supported\")"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "60d741dd-61b1-46ed-b791-3daeb39091e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:22:01.786174Z",
     "start_time": "2025-06-06T20:22:01.776285Z"
    }
   },
   "source": [
    "def predict(message, history, selected_model, system_message):\n",
    "    langchain_history = []\n",
    "    print(f\"***DEBUG Selected Model: {selected_model} \\n ***** History Object Type is: {type(history)} \\n***History Value: {history}\")\n",
    "\n",
    "    # Ensure history is a list. OpenAI is able to handle gr's history object but not Anthropic. Hence added the below code to make\n",
    "    # sure that the History object is iterable.\n",
    "    if not isinstance(history, list):\n",
    "        if hasattr(history, 'value'):\n",
    "            history = history.value\n",
    "        else:\n",
    "            history = []\n",
    "    model = get_model(selected_model)\n",
    "    print(f\"**** DEBUG Model Object is: {model}\")\n",
    "    \n",
    "    # history object doesn't contain System Message. We need to add it everytime\n",
    "    langchain_history.append(SystemMessage(content=system_message))\n",
    "    \n",
    "    for msg in history:\n",
    "        if msg['role'] == \"user\":\n",
    "            langchain_history.append(HumanMessage(content=msg['content']))\n",
    "        elif msg['role'] == \"assistant\":\n",
    "            langchain_history.append(AIMessage(content=msg['content']))\n",
    "    langchain_history.append(HumanMessage(content=message))\n",
    "    print(\"**** Debug LangChain history before invoke:\")\n",
    "    for msg in langchain_history:\n",
    "        print(f\" ****DEBUG  {type(msg).__name__}: {msg.content[:50]}...\")\n",
    "    llm_response = model.invoke(langchain_history)\n",
    "    \n",
    "    # Handle both string and object responses. OpenAI and Cla\n",
    "    if isinstance(llm_response, str):\n",
    "        # Llama 3.2 returns a str\n",
    "        response_content = llm_response\n",
    "    else:\n",
    "        # Needed for Claude and OpenAI\n",
    "        response_content = llm_response.content\n",
    "    \n",
    "    # Populate the history object with the latest user message and system response.\n",
    "    updated_history = history + [\n",
    "                        {\"role\": \"user\", \"content\": message},\n",
    "                        {\"role\": \"assistant\", \"content\": response_content}\n",
    "                    ] # We could have done history.append or history.extend but modifyinf history object doesn't always work. This definitely works.\n",
    "    print(f\"LLM Response: {llm_response} updated_history: {updated_history}\")\n",
    "    return \"\", updated_history, updated_history"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "e3202dd1-6350-426e-af67-e5868c0bfda8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:22:04.695921Z",
     "start_time": "2025-06-06T20:22:04.542015Z"
    }
   },
   "source": [
    "# define the UI\n",
    "with gr.Blocks() as multi_model_chat:\n",
    "    gr.Markdown(\"## 🧠 Multi-LLM Chatbot\")\n",
    "    with gr.Row():\n",
    "        model_selector = gr.Dropdown(\n",
    "            choices=[\"Llama: llama3.2\", \"Google: gemma3:1b\", \"OpenAI: gpt-4o-mini\", \"Claude: claude-sonnet-4-20250514\"\n",
    "                     , \"Google: gemini-2.0-flash\", \"Google: gemini-2.0-flash\"],\n",
    "            value=\"OpenAI: gpt-4o-mini\",\n",
    "            label=\"Choose LLM Model\",\n",
    "            multiselect=False,\n",
    "            interactive=True\n",
    "        )\n",
    "        system_message = gr.Textbox(placeholder=\"Enter optional system message here....\", label=\"System Message\", scale=4)\n",
    "        clear_btn = gr.Button(\"🧹 Clear Chat\")\n",
    "\n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", type = \"messages\")\n",
    "\n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(placeholder=\"Type your message here...\", label=\"Your Message\", scale=4)\n",
    "        send_btn = gr.Button(\"Send\", scale=1)\n",
    "\n",
    "    # Store history across messages\n",
    "    chat_history = gr.State([])\n",
    "\n",
    "    # If a new model is picked then all the chat boxes and history are all cleared.\n",
    "    system_message.input(lambda: ([], \"\", gr.State([])), outputs=[chatbot, user_input, chat_history])\n",
    "    \n",
    "    # When \"enter\" in key board is clicked after entering text in user_input data is submitted.\n",
    "    user_input.submit(\n",
    "        fn=predict,\n",
    "        inputs=[user_input, chat_history, model_selector, system_message],\n",
    "        outputs=[user_input, chatbot]  # Added chat_history to outputs\n",
    "    )\n",
    "    \n",
    "    # Button click triggers predict()\n",
    "    send_btn.click(\n",
    "        fn=predict,\n",
    "        inputs=[user_input, chat_history, model_selector, system_message],\n",
    "        outputs=[user_input, chat_history, chatbot]\n",
    "    )\n",
    "\n",
    "    # Clear button clears chat\n",
    "    clear_btn.click(lambda: ([], \"\", gr.State([])), outputs=[chatbot, user_input, chat_history])"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "019a7d37-59ce-41b6-b928-25f292319d0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:11:57.821433Z",
     "start_time": "2025-06-06T20:11:57.167859Z"
    }
   },
   "source": [
    "# Close the port if already running.\n",
    "try:\n",
    "    multi_model_chat.close()\n",
    "except:\n",
    "    pass  # Ignore if no server was running\n",
    "\n",
    "multi_model_chat.launch(inbrowser=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rameshUser\\.conda\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rameshUser\\.conda\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rameshUser\\.conda\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2181, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rameshUser\\.conda\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1692, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rameshUser\\.conda\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rameshUser\\.conda\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rameshUser\\.conda\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rameshUser\\.conda\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 889, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rameshUser\\AppData\\Local\\Temp\\ipykernel_11088\\749042472.py\", line 12, in predict\n",
      "    model = get_model(selected_model)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rameshUser\\AppData\\Local\\Temp\\ipykernel_11088\\1799847282.py\", line 5, in get_model\n",
      "    return ChatOpenAI(model=model_nm, api_key=os.getenv(\"OPENAI_API_KEY\"), temperature=0.7, timeout=30)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rameshUser\\.conda\\envs\\llms\\Lib\\site-packages\\langchain_core\\load\\serializable.py\", line 130, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"C:\\Users\\rameshUser\\.conda\\envs\\llms\\Lib\\site-packages\\pydantic\\main.py\", line 253, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rameshUser\\.conda\\envs\\llms\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 625, in validate_environment\n",
      "    self.root_client = openai.OpenAI(**client_params, **sync_specific)  # type: ignore[arg-type]\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rameshUser\\.conda\\envs\\llms\\Lib\\site-packages\\openai\\_client.py\", line 124, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c269baab-5128-468e-bd38-31f518619c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "# Placeholder stubs for LLM functions\n",
    "def call_openai(message, history):\n",
    "    return f\"🔵 OpenAI says: {message[::-1]}\"  # Fake response\n",
    "\n",
    "def call_claude(message, history):\n",
    "    return f\"🟡 Claude says: {message.upper()}\"  # Fake response\n",
    "\n",
    "def call_llama(message, history):\n",
    "    return f\"🟢 LLaMA says: {message.lower()}\"  # Fake response\n",
    "\n",
    "# Central function that routes based on model\n",
    "def predict(user_msg, chat_history, selected_model):\n",
    "    if selected_model == \"OpenAI (GPT-4)\":\n",
    "        reply = call_openai(user_msg, chat_history)\n",
    "    elif selected_model == \"Claude (Anthropic)\":\n",
    "        reply = call_claude(user_msg, chat_history)\n",
    "    elif selected_model == \"LLaMA (Ollama)\":\n",
    "        reply = call_llama(user_msg, chat_history)\n",
    "    else:\n",
    "        reply = \"❌ Unknown model selected.\"\n",
    "\n",
    "    chat_history.append((user_msg, reply))\n",
    "    return \"\", chat_history  # Clear textbox, return updated chat\n",
    "\n",
    "# Now define the UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## 🧠 Multi-LLM Chatbot\")\n",
    "\n",
    "    with gr.Row():\n",
    "        model_selector = gr.Dropdown(\n",
    "            choices=[\"OpenAI (GPT-4)\", \"Claude (Anthropic)\", \"LLaMA (Ollama)\"],\n",
    "            value=\"OpenAI (GPT-4)\",\n",
    "            label=\"Choose LLM Model\"\n",
    "        )\n",
    "        clear_btn = gr.Button(\"🧹 Clear Chat\")\n",
    "\n",
    "    chatbot = gr.Chatbot(label=\"Conversation\")\n",
    "\n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(placeholder=\"Type your message here...\", label=\"Your Message\", scale=4)\n",
    "        send_btn = gr.Button(\"Send\", scale=1)\n",
    "\n",
    "    # Store history across messages\n",
    "    chat_history = gr.State([])\n",
    "\n",
    "    # Button click triggers predict()\n",
    "    send_btn.click(\n",
    "        fn=predict,\n",
    "        inputs=[user_input, chat_history, model_selector],\n",
    "        outputs=[user_input, chatbot]\n",
    "    )\n",
    "\n",
    "    # Clear button clears chat\n",
    "    clear_btn.click(lambda: ([], \"\"), outputs=[chatbot, user_input])\n",
    "\n",
    "demo.launch()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c49c7-4d8d-4960-8bfb-6b1792816079",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def predict(message, history):\n",
    "    print(f\"History Object is: {history} ***\")\n",
    "    history_langchain_format = []\n",
    "    for msg in history:\n",
    "        if msg['role'] == \"user\":\n",
    "            history_langchain_format.append(HumanMessage(content=msg['content']))\n",
    "        elif msg['role'] == \"assistant\":\n",
    "            history_langchain_format.append(AIMessage(content=msg['content']))\n",
    "    history_langchain_format.append(HumanMessage(content=message))\n",
    "    gpt_response = model.invoke(history_langchain_format)\n",
    "    return gpt_response.content\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    predict,\n",
    "    type=\"messages\"\n",
    ")\n",
    "\n",
    "demo.launch()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
