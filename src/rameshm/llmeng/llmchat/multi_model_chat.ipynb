{
 "cells": [
  {
   "cell_type": "code",
   "id": "a653e8a9-9917-4623-a197-f9f52cc4cf2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T20:29:28.697388Z",
     "start_time": "2025-06-07T20:29:28.691244Z"
    }
   },
   "source": [
    "# This is a simple general-purpose chatbot built on top of LangChain and Gradio.\n",
    "# Before running this, make sure you have:\n",
    "# 1. retrieved Keys for the LLM model environment you want to run\n",
    "# 2. set environment variable LLM_KEY_FILE pointing to the full path that contains the keys for the LLM Models."
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "af84394f-f1ad-46e3-83c4-4bf22e42e5fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T20:29:38.060265Z",
     "start_time": "2025-06-07T20:29:28.718557Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "import gradio as gr\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from rameshm.llmeng.utils import init_utils\n",
    "from rameshm.llmeng.utils.init_utils import set_environment_logger"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a732616a-8b2b-453d-8cac-764c9eefb4b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T20:29:38.997134Z",
     "start_time": "2025-06-07T20:29:38.976474Z"
    }
   },
   "source": [
    "# Load my environment\n",
    "logger = set_environment_logger()\n",
    "print(f\"OPENAI_API_KEY: {os.getenv('OPENAI_API_KEY')}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log File: c:\\temp\\my_logs.txt\n",
      "OPENAI_API_KEY: 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\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "09b74bb1-c4a0-411c-b5b9-54259af61b80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T20:31:54.688883Z",
     "start_time": "2025-06-07T20:31:54.679337Z"
    }
   },
   "source": [
    "def get_model(model_nm: str):\n",
    "    model_nm = model_nm.split(\" \")[1].strip()\n",
    "    print(f\"Using Model: {model_nm}\")\n",
    "    if \"gpt\" in model_nm:\n",
    "        return ChatOpenAI(model=model_nm, api_key=os.getenv(\"OPENAI_API_KEY\"), temperature=0.7, timeout=30)\n",
    "    elif \"claude\" in model_nm:\n",
    "        return ChatAnthropic(model=model_nm, api_key=os.getenv(\"ANTHROPIC_API_KEY\"), timeout=30,\n",
    "                             temperature=0.7, max_tokens=1024,top_p=0.9, top_k=40)\n",
    "    elif \"llama\" in model_nm or \"gemma\" in model_nm:\n",
    "        # Ollama run on \"http://localhost:11434\"  # Default Ollama URL. If you type that URL you shoud see \"Ollama Running\" message\n",
    "        return Ollama(model=model_nm, # api_key=\"ollama\",base_url=\"http://localhost:11434\", \n",
    "                      temperature=0.7, top_p=0.9, top_k=40, num_predict=256, repeat_penalty=1.1)\n",
    "    elif \"gemini\" in model_nm:\n",
    "         return ChatGoogleGenerativeAI(model=model_nm, google_api_key=os.getenv(\"GOOGLE_API_KEY\"), timeout=30)\n",
    "    else:\n",
    "        raise Exception(\"Model: {model_nm} is not supported\")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T20:32:20.564689Z",
     "start_time": "2025-06-07T20:32:20.553602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(message, history, selected_model, system_message):\n",
    "    langchain_history = []\n",
    "    print(f\"***DEBUG Selected Model: {selected_model} \\n ***** History Object Type is: {type(history)} \\n***History Value: {history}\")\n",
    "\n",
    "    # Ensure history is a list. OpenAI is able to handle gr's history object but not Anthropic. Hence added the below code to make\n",
    "    # sure that the History object is iterable.\n",
    "    if not isinstance(history, list):\n",
    "        if hasattr(history, 'value'):\n",
    "            history = history.value\n",
    "        else:\n",
    "            history = []\n",
    "    model = get_model(selected_model)\n",
    "    print(f\"**** DEBUG Model Object is: {model}\")\n",
    "\n",
    "    # history object doesn't contain System Message. We need to add it everytime\n",
    "    langchain_history.append(SystemMessage(content=system_message))\n",
    "\n",
    "    for msg in history:\n",
    "        if msg['role'] == \"user\":\n",
    "            langchain_history.append(HumanMessage(content=msg['content']))\n",
    "        elif msg['role'] == \"assistant\":\n",
    "            langchain_history.append(AIMessage(content=msg['content']))\n",
    "    langchain_history.append(HumanMessage(content=message))\n",
    "    print(\"**** Debug LangChain history before invoke:\")\n",
    "    for msg in langchain_history:\n",
    "        print(f\" ****DEBUG  {type(msg).__name__}: {msg.content[:50]}...\")\n",
    "    llm_response = model.invoke(langchain_history)\n",
    "\n",
    "    # Handle both string and object responses. OpenAI and Cla\n",
    "    if isinstance(llm_response, str):\n",
    "        # Llama 3.2 returns a str\n",
    "        response_content = llm_response\n",
    "    else:\n",
    "        # Needed for Claude and OpenAI\n",
    "        response_content = llm_response.content\n",
    "\n",
    "    # Populate the history object with the latest user message and system response.\n",
    "    updated_history = history + [\n",
    "                        {\"role\": \"user\", \"content\": message},\n",
    "                        {\"role\": \"assistant\", \"content\": response_content}\n",
    "                    ] # We could have done history.append or history.extend but modifyinf history object doesn't always work. This definitely works.\n",
    "    print(f\"LLM Response: {llm_response} updated_history: {updated_history}\")\n",
    "    return \"\", updated_history, updated_history"
   ],
   "id": "60d741dd-61b1-46ed-b791-3daeb39091e0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T20:32:22.449441Z",
     "start_time": "2025-06-07T20:32:22.280080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define the UI\n",
    "with gr.Blocks() as multi_model_chat:\n",
    "    gr.Markdown(\"## 🧠 Multi-LLM Chatbot\")\n",
    "    with gr.Row():\n",
    "        model_selector = gr.Dropdown(\n",
    "            choices=[\"Llama: llama3.2\", \"Google: gemma3:1b\", \"OpenAI: gpt-4o-mini\", \"Claude: claude-sonnet-4-20250514\"\n",
    "                     , \"Google: gemini-2.0-flash\", \"Google: gemini-2.0-flash\"],\n",
    "            value=\"OpenAI: gpt-4o-mini\",\n",
    "            label=\"Choose LLM Model\",\n",
    "            multiselect=False,\n",
    "            interactive=True\n",
    "        )\n",
    "        system_message = gr.Textbox(placeholder=\"Enter optional system message here....\", label=\"System Message\", scale=4)\n",
    "        clear_btn = gr.Button(\"🧹 Clear Chat\")\n",
    "\n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", type = \"messages\")\n",
    "\n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(placeholder=\"Type your message here...\", label=\"Your Message\", scale=4)\n",
    "        send_btn = gr.Button(\"Send\", scale=1)\n",
    "\n",
    "    # Store history across messages\n",
    "    chat_history = gr.State([])\n",
    "\n",
    "    # If a new model is picked then all the chat boxes and history are all cleared.\n",
    "    system_message.input(lambda: ([], \"\", gr.State([])), outputs=[chatbot, user_input, chat_history])\n",
    "\n",
    "    # When \"enter\" in key board is clicked after entering text in user_input data is submitted.\n",
    "    user_input.submit(\n",
    "        fn=predict,\n",
    "        inputs=[user_input, chat_history, model_selector, system_message],\n",
    "        outputs=[user_input, chatbot]  # Added chat_history to outputs\n",
    "    )\n",
    "\n",
    "    # Button click triggers predict()\n",
    "    send_btn.click(\n",
    "        fn=predict,\n",
    "        inputs=[user_input, chat_history, model_selector, system_message],\n",
    "        outputs=[user_input, chat_history, chatbot]\n",
    "    )\n",
    "\n",
    "    # Clear button clears chat\n",
    "    clear_btn.click(lambda: ([], \"\", gr.State([])), outputs=[chatbot, user_input, chat_history])"
   ],
   "id": "e3202dd1-6350-426e-af67-e5868c0bfda8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "019a7d37-59ce-41b6-b928-25f292319d0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T20:32:29.346303Z",
     "start_time": "2025-06-07T20:32:28.745116Z"
    }
   },
   "source": [
    "# Close the port if already running.\n",
    "try:\n",
    "    multi_model_chat.close()\n",
    "except:\n",
    "    pass  # Ignore if no server was running\n",
    "\n",
    "multi_model_chat.launch(inbrowser=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c269baab-5128-468e-bd38-31f518619c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "# Placeholder stubs for LLM functions\n",
    "def call_openai(message, history):\n",
    "    return f\"🔵 OpenAI says: {message[::-1]}\"  # Fake response\n",
    "\n",
    "def call_claude(message, history):\n",
    "    return f\"🟡 Claude says: {message.upper()}\"  # Fake response\n",
    "\n",
    "def call_llama(message, history):\n",
    "    return f\"🟢 LLaMA says: {message.lower()}\"  # Fake response\n",
    "\n",
    "# Central function that routes based on model\n",
    "def predict(user_msg, chat_history, selected_model):\n",
    "    if selected_model == \"OpenAI (GPT-4)\":\n",
    "        reply = call_openai(user_msg, chat_history)\n",
    "    elif selected_model == \"Claude (Anthropic)\":\n",
    "        reply = call_claude(user_msg, chat_history)\n",
    "    elif selected_model == \"LLaMA (Ollama)\":\n",
    "        reply = call_llama(user_msg, chat_history)\n",
    "    else:\n",
    "        reply = \"❌ Unknown model selected.\"\n",
    "\n",
    "    chat_history.append((user_msg, reply))\n",
    "    return \"\", chat_history  # Clear textbox, return updated chat\n",
    "\n",
    "# Now define the UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## 🧠 Multi-LLM Chatbot\")\n",
    "\n",
    "    with gr.Row():\n",
    "        model_selector = gr.Dropdown(\n",
    "            choices=[\"OpenAI (GPT-4)\", \"Claude (Anthropic)\", \"LLaMA (Ollama)\"],\n",
    "            value=\"OpenAI (GPT-4)\",\n",
    "            label=\"Choose LLM Model\"\n",
    "        )\n",
    "        clear_btn = gr.Button(\"🧹 Clear Chat\")\n",
    "\n",
    "    chatbot = gr.Chatbot(label=\"Conversation\")\n",
    "\n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(placeholder=\"Type your message here...\", label=\"Your Message\", scale=4)\n",
    "        send_btn = gr.Button(\"Send\", scale=1)\n",
    "\n",
    "    # Store history across messages\n",
    "    chat_history = gr.State([])\n",
    "\n",
    "    # Button click triggers predict()\n",
    "    send_btn.click(\n",
    "        fn=predict,\n",
    "        inputs=[user_input, chat_history, model_selector],\n",
    "        outputs=[user_input, chatbot]\n",
    "    )\n",
    "\n",
    "    # Clear button clears chat\n",
    "    clear_btn.click(lambda: ([], \"\"), outputs=[chatbot, user_input])\n",
    "\n",
    "demo.launch()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
