{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-06T13:42:39.366154Z",
     "start_time": "2025-06-06T13:42:27.675003Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "from langchain_community.chat_models.friendli import get_chat_request\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "import gradio as gr\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dotenv import load_dotenv\n",
    "from sympy import false\n",
    "import random\n",
    "\n",
    "from rameshm.llmeng.llmchat.llm_chat import LlmChat\n",
    "from rameshm.llmeng.utils.init_utils import set_environment_logger"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T13:46:31.421458Z",
     "start_time": "2025-06-06T13:46:31.399544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.environ[\"LLM_KEY_FILE\"] = \"c:\\learning\\llm\\projects\\llm_engineering\\.env\"\n",
    "logger = set_environment_logger()"
   ],
   "id": "af02ad85946c754",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log File: c:\\temp\\my_logs.txt\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T13:47:02.864319Z",
     "start_time": "2025-06-06T13:47:02.836518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_model(model_nm: str):\n",
    "    model_nm = model_nm.split(\" \")[1].strip()\n",
    "    print(f\"Using Model: {model_nm}\")\n",
    "    if \"gpt\" in model_nm:\n",
    "        return ChatOpenAI(model=model_nm, api_key=os.getenv(\"OPENAI_API_KEY\"), temperature=0.7, timeout=30)\n",
    "    elif \"claude\" in model_nm:\n",
    "        return ChatAnthropic(model=model_nm, api_key=os.getenv(\"ANTHROPIC_API_KEY\"), timeout=30,\n",
    "                             temperature=0.7, max_tokens=1024,top_p=0.9, top_k=40)\n",
    "    elif \"llama\" in model_nm or \"gemma\" in model_nm:\n",
    "        # Ollama run on \"http://localhost:11434\"  # Default Ollama URL. If you type that URL you shoud see \"Ollama Running\" message\n",
    "        return Ollama(model=model_nm, # api_key=\"ollama\",base_url=\"http://localhost:11434\",\n",
    "                      temperature=0.7, top_p=0.9, top_k=40, num_predict=256, repeat_penalty=1.1)\n",
    "    elif \"gemini\" in model_nm:\n",
    "         return ChatGoogleGenerativeAI(model=model_nm, google_api_key=os.getenv(\"GOOGLE_API_KEY\"), timeout=30)\n",
    "    else:\n",
    "        raise Exception(\"Model: {model_nm} is not supported\")"
   ],
   "id": "39ef1d76f868bbc2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T13:48:13.036286Z",
     "start_time": "2025-06-06T13:48:13.019775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate_inputs(message: str, history: List, model: str, system_message: str) -> tuple:\n",
    "    \"\"\"Validate inputs before processing\"\"\"\n",
    "    if not message or not message.strip():\n",
    "        logger.error(\"Please enter a message\")\n",
    "        return false, \"Please enter a message\"\n",
    "\n",
    "    if not model:\n",
    "        logger.error(\"Please enter a model\")\n",
    "        return False, \"Please select a model\"\n",
    "\n",
    "    # Check if required API keys are available\n",
    "    model_name = model.lower()\n",
    "    #  Ollama runs locally, so no API key is needed\n",
    "    if \"gpt\" in model_name and not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        msg = \"OpenAI API key not found in environment variables\"\n",
    "        logger.error(msg)\n",
    "        return False, msg\n",
    "    elif \"claude\" in model_name and not os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "        msg = \"Anthropic API key not found in environment variables\"\n",
    "        logger.error(msg)\n",
    "        return False, msg\n",
    "    elif \"gemini\" in model_name and not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "        msg = \"Google API key not found in environment variables\"\n",
    "        logger.error(msg)\n",
    "        return False, msg\n",
    "    return True, \"\""
   ],
   "id": "4804849195708342",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T13:50:07.444154Z",
     "start_time": "2025-06-06T13:50:07.423328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(message: str, history: List, selected_model: str, system_message: str,\n",
    "            current_chat_id: Optional[str], chat_list: List[Dict[str, LlmChat]]) -> Tuple[str, List, List, str, List]:\n",
    "    \"\"\"Enhanced predict function with chat management\"\"\"\n",
    "    start_time = time.time()\n",
    "    chat_list_modified = chat_list\n",
    "    err_msg = None  # set to string if we encounter an error\n",
    "\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        is_valid, error_msg = validate_inputs(message, history, selected_model, system_message)\n",
    "        if not is_valid:\n",
    "            logger.error(f\"Validation failed: {error_msg}\")\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        logger.info(f\"Processing request - Model: {selected_model}, Message length: {len(message)}\")\n",
    "\n",
    "        # Ensure history is a list\n",
    "        if not isinstance(history, list):\n",
    "            history = history.value if hasattr(history, 'value') else []\n",
    "\n",
    "        # Initialize model\n",
    "        model = get_model(selected_model)\n",
    "        logger.info(f\"Model initialized: {type(model).__name__}\")\n",
    "\n",
    "        # Build langchain history\n",
    "        langchain_history = []\n",
    "\n",
    "        # Add system message if provided\n",
    "        if system_message and system_message.strip():\n",
    "            langchain_history.append(SystemMessage(content=system_message.strip()))\n",
    "            logger.info(f\"Added system message ({len(system_message)} chars)\")\n",
    "\n",
    "        # Add conversation history\n",
    "        for i, msg in enumerate(history):\n",
    "            try:\n",
    "                if msg.get('role') == \"user\":\n",
    "                    langchain_history.append(HumanMessage(content=msg['content']))\n",
    "                elif msg.get('role') == \"assistant\":\n",
    "                    langchain_history.append(AIMessage(content=msg['content']))\n",
    "            except Exception as e:\n",
    "                err_msg = f\"Malformed message history at index {i}: {msg}\"\n",
    "                logger.error(err_msg)\n",
    "                raise ValueError(err_msg) from e\n",
    "\n",
    "        # Add current user message\n",
    "        langchain_history.append(HumanMessage(content=message))\n",
    "\n",
    "        logger.debug(f\"Built conversation with {len(langchain_history)} messages\")\n",
    "        logger.debug(f\"Making API call with model: {selected_model} with total messages: {len(langchain_history)}\")\n",
    "\n",
    "        # Make API call with timeout handling\n",
    "        llm_response = model.invoke(langchain_history)\n",
    "        elapsed = time.time() - start_time\n",
    "        logger.debug(f\"API call completed in {elapsed:.2f}s\")\n",
    "\n",
    "        # Handle response format differences\n",
    "        if isinstance(llm_response, str):\n",
    "            response_content = llm_response\n",
    "        else:\n",
    "            response_content = getattr(llm_response, 'content', str(llm_response))\n",
    "\n",
    "        if not response_content:\n",
    "            err_msg = \"Received empty response from model: {selected_model} for message: {message}\"\n",
    "            logger.error(err_msg)\n",
    "            raise ValueError(err_msg)\n",
    "        else:\n",
    "            logger.info(f\"Received response: {response_content}\\n\")\n",
    "\n",
    "        # Update conversation history\n",
    "        updated_history = history + [\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "            {\"role\": \"assistant\", \"content\": response_content}\n",
    "        ]\n",
    "\n",
    "        # Save/update chat\n",
    "        if not current_chat_id:\n",
    "            current_chat_id = generate_chat_id()\n",
    "            current_llm_chat = LlmChat(chat_id=current_chat_id, history=updated_history,\n",
    "                                       model_nm=selected_model, system_message=system_message)\n",
    "            chat_list_modified += [{\"chat_id\": current_chat_id, \"llm_chat\": current_llm_chat}]\n",
    "        else:\n",
    "            # Update current with updated chat\n",
    "            current_llm_chat = next((chat['llm_chat'] for chat in chat_list if chat['chat_id'] == current_chat_id), None)\n",
    "            current_llm_chat.update_chat_history(updated_history)\n",
    "\n",
    "        logger.debug(f\"ðŸ’¬ Response: {response_content[:200]}{'...' if len(response_content) > 200 else ''}\")\n",
    "        return \"\", updated_history, updated_history, current_chat_id, get_chat_list(chat_list_modified),\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        logger.error(f\"Unexpected error after {elapsed:.2f}s: {e}\")\n",
    "        error_response = [\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "            {\"role\": \"assistant\", \"content\": f\"ðŸ”¥ ERROR Exception: Unexpected error: {str(e)}\"}\n",
    "        ]\n",
    "        updated_history = history + error_response\n",
    "        return \"\", updated_history, updated_history, current_chat_id or \"\", get_chat_list(chat_list_modified),\n"
   ],
   "id": "1c08ce72fe15127a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6bb7b6f742ce9c1e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
