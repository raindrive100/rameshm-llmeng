apiVersion: apps/v1
kind: Deployment
metadata:
  name: llmchat-deployment
  labels:
    app: llmchat
spec:
  replicas: 1 # Start with 1 replica for  LLMChat app
  selector:
    matchLabels:
      app: llmchat
  revisionHistoryLimit: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  template:  # Pod definition starts
    metadata:
      labels:
        app: llmchat
    spec:
      containers:
      - name: llmchat
        image: us-east4-docker.pkg.dev/myproject-07072025/llmengineering/llmchat:latest
        resources: # Define some resources. We don't need much because most of the work is done by LLMs.
          requests:
            cpu: "2"    # Request 4 CPU cores
            memory: "8Gi" # Request 8 GiB of memory
          limits:
            cpu: "4"    # Allow up to 6 CPU cores
            memory: "16Gi" # Allow up to 16 GiB of memory
        ports:
        - containerPort: 7860 # Gradio app port
        env: # You can still set this env var directly here
          - name: OLLAMA_HOST # Connection to ollama hosted models use this environment variable to connect to Ollama server
            value: http://ollama-service:11434 # Reference the Ollama Kubernetes Service name
        envFrom: # Loading a mount point of the environment file to keep code consistent between Dev and Prod.
        - secretRef:
            name: llmeng-secrets # Inject environment variables from your .env secret